{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment1_Module2_Preprocessing.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u30C2r3hCZVk"
      },
      "source": [
        "#Introduction\n",
        "\n",
        "In this assignment, we go through text preprocessing. Materials that we saw in module 2, will be tested here. \n",
        "\n",
        "In this assignment, we mostly work with Spacy library. This is a small assignment. For any problem that you face, please email me.\n",
        "\n",
        "Parts that have to be filled are marked with @TODO (7 small @TODOs)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gtSOshkn-tzv"
      },
      "source": [
        "# Downloading and unzipping dataset\n",
        "\n",
        "The dataset that we developed here is product reviews on Amazon. As an example, we download reviews of office products, but you can alternatively, download the corresponding file for any other product, from the website [\"Amazon reviews dataset\"](http://jmcauley.ucsd.edu/data/amazon/) , in \"small subsets for experimentation\", of 5-core files.\n",
        "\n",
        "If you are running this on google colab, \"!\" lines will run bash commands. If you want to run this code on your machine, you can download and unzip the dataset manually, and unzip it. Then in the next cell, give the address of your file We download and unzip the dataset with bash commands: wget and gzip. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UEH2VCWX-eQC"
      },
      "source": [
        "!wget http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Office_Products_5.json.gz\n",
        "!gzip -d reviews_Office_Products_5.json.gz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SprAMAaEiYGc"
      },
      "source": [
        "Once we download, and unzipped product reviews, we can read all lines into a list, that is productsData. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eWdfRgIT-6st"
      },
      "source": [
        "import json\n",
        "f = open(\"reviews_Office_Products_5.json\", \"r\")\n",
        "productsData = [json.loads(line) for line in f.readlines()]\n",
        "\n",
        "print(productsData[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eu7N_DD5jzI-"
      },
      "source": [
        "Once we read all lines of dataset, we shall take two important key-value from each line. We take out \"reviewText\",  and \"overall\". reviewText will be processed to be used as our input features, and we extract sentiments from overall key.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yGYmDj8Z_me6"
      },
      "source": [
        "reviews = []\n",
        "sentiments = []\n",
        "for product in productsData: #for each line in the json file\n",
        "    \n",
        "    if product[\"reviewText\"] == \"\": #if the review is empty, ignore that line, otherwise:\n",
        "        continue\n",
        "    \n",
        "    reviews.append(product[\"reviewText\"]) #put reviewText in a reviews list\n",
        "\n",
        "    #now making the sentiment analysis label. We can say that if the rating of a review is above 3, so the sentiment of review is positive, else, the sentiment is negative.\n",
        "    if product[\"overall\"] > 3:\n",
        "        sentiments.append(\"positive\")\n",
        "    else:\n",
        "        sentiments.append(\"negative\")\n",
        "\n",
        "#print out some in dataset\n",
        "for i in range(50):\n",
        "    print(sentiments[i], \"\\t\", reviews[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PqzXMBZhmqyY"
      },
      "source": [
        "**@TODO_1** In the printed samples, it seems the number of positive and negative samples are not balanced. Do you think it might be problematic? Any solution that comes to your mind?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8xQ0MN4hnQLE"
      },
      "source": [
        "# Preprocessing\n",
        "\n",
        "In this section we do some preprocessing by Spacy library. Spacy is one of the most famous library in natural language, alongside NLTK, and Stanford NLP libraries. For each application, you might find one model giving a better performance than others. In video M2_1E, we have seen how to deploy NLTK; Stanford library was originally written in Java, and most of NLP applications in Java use that. However, recently there have been some attempt for having this library in python [(stanfordNLPPython)](https://github.com/stanfordnlp/stanza/), but still I personally use it with running a Java server; Spacy gives many different linguistic features, and using it is quite simple. Spacy has different models. We use sm (small) model in this assignment. Noting that as small as model become, as faulty as it become. But using larger models, is significantly slower. \n",
        "\n",
        "For using Spacy model, we have to load a spacy model, in this assignment, English small model. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G1XzgokqA3T6"
      },
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm') \n",
        "#create a spacy document nlp by: doc= nlp(\"your sentence\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VmeWG4YprrQr"
      },
      "source": [
        "##Noun chunks\n",
        "Playing around with Spacy, taking out noun chunks from each sentence. For simplicity, only the first few reviews are processed. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XGdYRpW2AwQl"
      },
      "source": [
        "#noun chunks\n",
        "nounChunks = []\n",
        "for review in reviews[:10]:\n",
        "   doc = nlp(review) # this way, you can make a spacy document for each sentence\n",
        "   tmpNounChunks = doc.noun_chunks\n",
        "\n",
        "   nounChunks.append([chunk.text for chunk in tmpNounChunks])\n",
        "\n",
        "print(nounChunks[:10])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P4pRBxvj_KxF"
      },
      "source": [
        "## N-grams\n",
        "We have seen n-grams in video lectures. Now lets see how we can compute n-grams of our dataset. \n",
        "\n",
        "For computing n-grams of dataset, we move a window of n-token on our dataset, that it generates a list of two-grams for each sentence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J4Ke9KVsGzL_"
      },
      "source": [
        "#n-grams\n",
        "n = 2\n",
        "n_grams = [] #all n-grams\n",
        "for review in reviews[:10]: #for only the first few sentences(reviews) in the dataset\n",
        "    doc = nlp(review) #create a nlp document\n",
        "    sentence = [token.text for token in doc] #getting tokens text\n",
        "    grams = [sentence[i:i+n] for i in range(len(sentence)-n+1)] # create list of n-tokens\n",
        "    n_grams.append(grams)\n",
        "print(n_grams[:10])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZIlEsZ7_ACtS"
      },
      "source": [
        "## Linguistic features\n",
        "Spacy provides some of linguistic features easily. In the following cell we print linguistic features that Spacy provides. \n",
        "\n",
        "One interesting thing to check is that if we want to take POS of each review, the computation time becomes long. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z5KeZiQmcNls"
      },
      "source": [
        "doc = nlp(reviews[0])\n",
        "print(\"text\\tlemma\\tpos\\ttag\\tdep\\tshape\\tis_alpha\\tis_stop\")\n",
        "for token in doc:\n",
        "    \n",
        "    features = [token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n",
        "            token.shape_, str(token.is_alpha), str(token.is_stop)]\n",
        "    print(\"\\t\".join(features))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_-eXCl1kBD4G"
      },
      "source": [
        "## The model you call is important\n",
        "One of the practicality in processing (when the label is sentence-wise), is to use base form of each token. This makes the input simpler, and the size of vocabulary smaller. In the following cell we see the lemma of the first sentence in our dataset. We know that lemma of each token is the base form of token (that is lower cased).\n",
        "\n",
        "In the following cell, we call two different Spacy model, and check the lemma of both. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r03NDLrYVgZ3"
      },
      "source": [
        "from spacy.lang.en import English\n",
        "nlp1 = English()\n",
        "nlp2 = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "testSentence = \"Natural language processing (NLP) is a branch of artificial intelligence that helps computers understand, interpret and manipulate human language\"\n",
        "doc1 = nlp1(testSentence)\n",
        "doc2 = nlp2(testSentence)\n",
        "for i in range(len(doc1)):\n",
        "    print(doc1[i].text, \" ::: \", doc1[i].lemma_, \" ::: \", doc2[i].lemma_)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "otFqleA67-Do"
      },
      "source": [
        "As we saw, one of the models, instead of lemma_, only gives the original word form. This is probably becuase the model is smaller, and does not give all functionalities."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0xaPpIIdBfm2"
      },
      "source": [
        "One of the linguistic features that we learned about in module 2, is the dependency relation between words in a sentence. Spacy gives these relations in the shape of a tree (directed graph). Passing a sentence into NLP, spacy compute this tree, and we can use displacy to visualize this tree.\n",
        "\n",
        "Spacy can also visualize Named Entity Recognition of a sentence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fLxmaXJyY1s1"
      },
      "source": [
        "from spacy import displacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(\"A test sentence for Smarter NLP course\")\n",
        "#dependency tree visualizer\n",
        "displacy.render(doc, style=\"dep\", jupyter=True)\n",
        "##Named Entity Recognition\n",
        "displacy.render(doc, style=\"ent\", jupyter=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "teDbVOLrhMcM"
      },
      "source": [
        "**@TODO_2** Try different sentences and visualize the dependency tree. Do you think dependency tree is always reliable? Try to change small tokens, and see if the result of dependency tree randomly changes or not."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E1VOupleNqCe"
      },
      "source": [
        "#Making language computable\n",
        "\n",
        "Our downloaded dataset is a list of strings. We have to translate it into list of numbers. In this section we will see how to do it. \n",
        "\n",
        "At the first step, for each review, we create a Spacy object, and from that we process token by token. For each of tokens that is not a stop word, and is made of alphabets, we add the lemma of this token to a temporary list of tokens. Once we processed all words of a review, add the temporary list of tokens to our new dataset. \n",
        "\n",
        "Most often, in datasets there are some sentences that are lengthy. A common approach is to cut those sentence to a number (max_len) that shorten those few sentences, and keep the most of dataset the same.\n",
        "\n",
        "**@TODO_3** replace ##### with correct code.\n",
        "\n",
        "**@TODO_4** After filling the code, we can try different spacy models. As we already saw, Spacy in different models do not give the lemma of word. Try each spacy model, and write which do you prefer? Do you see a time difference?\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PttGneBw_t5M"
      },
      "source": [
        "from spacy.lang.en import English\n",
        "\n",
        "#@TODO_4  choose one of the two nlp model, and test\n",
        "nlp = English()\n",
        "#nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "cleanedTokenizedReviews = []\n",
        "\n",
        "max_len = 400 # msximum allowed length of each sentence/review\n",
        "\n",
        "\n",
        "##### #for each review in reviews\n",
        "    ##### # create a spacy nlp doc\n",
        "    reviewTokens = [] #initialize a temporary list\n",
        "    ##### #for each token in review\n",
        "        ##### #if the token is not is_stop and is_alpha, so\n",
        "            ##### #append token.lemma_.lower() to our temporary list. Don't forget to lowercase each lemma, as it make the size of dictionary smaller.\n",
        "\n",
        "    #cut the lengthy sentences, to max_len tokens\n",
        "    if len(reviewTokens) > max_len: #if the length of sentence is above maximum length\n",
        "        reviewTokens = reviewTokens[:max_len] # taking max_len number of tokens only\n",
        "    cleanedTokenizedReviews.append(reviewTokens) #append each review(tokenized and cleaned) to make a new dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MIrkkgOH2YM3"
      },
      "source": [
        "#print out some data to see how it looks\n",
        "cleanedTokenizedReviews[0] #it should be a list of tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7aRAUnRPYNR5"
      },
      "source": [
        "## Vocabulary\n",
        "Vocabulary is an indexed container for each individual tokens. Often, we have to add dummy tokens to beginning and end of sentences. In that case, we reserve these dummy tokens in our vocabulary. We reserve index Zero in our vocabulary, <sos>, that is Start Of Setence to the begining of each sentence; <EOS>, that is End Of Sentence, to the end of each sentence.\n",
        "\n",
        "There are different containers for saving a vocabulary, e.g. list or dictionary. Most often, dictionary is used, since it is faster, and can handle larger data in memory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RV-6E4HP_y-l"
      },
      "source": [
        "vocabulary = {}\n",
        "\n",
        "vocabulary[\"<sos>\"] = 1\n",
        "vocabulary[\"<eos>\"] = 2\n",
        "\n",
        "for review in cleanedTokenizedReviews: #for each review in reviews\n",
        "    for token in review: #for each token in review\n",
        "        if vocabulary.get(token) == None: #if that token is not in the vocabulary\n",
        "            vocabulary[token] = len(vocabulary)+1 #add a key-value of token-integer\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nKV6Keecanr6"
      },
      "source": [
        "We also need inversed vocabulary. Inversed vocabulary is used when we have indices (e.g. prediction of Machine learning models) and want to obtain the original tokens."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yIOyoRq2_z-Q"
      },
      "source": [
        "inverseVocab = {item[1]:item[0] for item in vocabulary.items()} #inverse of vocabulary, that is integer-token"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BKSjjVpYbKm7"
      },
      "source": [
        "##Ordinal encoding\n",
        "Once we computed vocabulary, so we can replace each token in our dataset, by its index in the dictionry. \n",
        "\n",
        "**@TODO_5** replace ##### with a line of code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P1K5y-JN_4nG"
      },
      "source": [
        "ordinalReviews = [] #creating a list for new dataset\n",
        "##### # for each review in cleanedTokenizedReviews\n",
        "    ordinalReview = [] #a list for ordinal encoding of review\n",
        "    ##### #for each token in review\n",
        "        ##### #append vocabulary[token] to ordinalReview\n",
        "    ##### # append each ordinalized review to ordinalReviews"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_oouaJvMbbOi"
      },
      "source": [
        "## Zero padding\n",
        "For a learning machine, we need to make the length of all sentences the same. This process is called zero padding. We assume a maximum length for each sentence, and add zeros the the begining (or to the end) of each sentence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wu4L8RcP_629"
      },
      "source": [
        "#zero padding\n",
        "paddedReviews = []\n",
        "for review in ordinalReviews: #for each review in reviews\n",
        "    pad = [0]*max_len # make a pad of zeros (fixed size)\n",
        "    pad[-len(review):] = review #that our sentence fits at the most end of this pad\n",
        "    paddedReviews.append(pad) #append to create a new dataset\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8jhNYbA7cvbM"
      },
      "source": [
        "And finally, translate our dataset, into an array of shape numberOfReview*MaximumLength"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W-HJC2h9_95z"
      },
      "source": [
        "import numpy as np\n",
        "X = np.array([np.array(review) for review in paddedReviews]).reshape((len(paddedReviews), max_len))\n",
        "print(X.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tclfx63JdAh-"
      },
      "source": [
        "##labels\n",
        "But Don't forget the labels. Labels for sentiment analysis are per-sentence, and we can modify them at the end. For computing labels, we replaces positive with integer 1, and negative with integer 0. \n",
        "\n",
        "**@TODO_6** Replace ##### with a line of code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ROAxa5A0__vk"
      },
      "source": [
        "#labels\n",
        "y = []\n",
        "##### #for each sentiment in sentiments\n",
        "    ##### # if it is positive append 1 to y\n",
        "        #####\n",
        "    #####\n",
        "        ##### #else, append 0 to y\n",
        "        \n",
        "y = np.asarray(y)   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FkZ6qHPvVVmM"
      },
      "source": [
        "#Conclusion\n",
        " **@TODO_7** Answer the following questions \n",
        "\n",
        "This notebook showed a simple solution to text preprocessing, before passing text to learning machines. the code of above, was simple, and from basics. There are many libraries that can do each of these tasks, simpler and faster. \n",
        "\n",
        "1. Can you find a replacement for zeropadding? \n",
        "\n",
        "2. Take a look into https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing . Which functions you find useful for text preprocessing? \n",
        "\n",
        "3. We did not cover one-hot encoding here. Use the function keras.utils.to_categorical to translate y (ordinal encoding), to one hot encoding. \n",
        "\n",
        "We noticed time in preprocessing might become a critical factor. Deep learning frameworks provided some solutions for that. If you are interested into more sophisticated ways of text preprocessing, read the following links. \n",
        "\n",
        "a. https://keras.io/guides/preprocessing_layers\n",
        "\n",
        "b. https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text_dataset_from_directory\n",
        "\n"
      ]
    }
  ]
}